---
title: Analysis at Scale
narratives:
---

**Scenario:**

I am working with a really large and complex dataset, stored across
multiple locations.  I have a workflow that I run on this dataset and
I’d like to update it as new data is added. I don’t have the memory or
compute power to do this as a robust process, and the data is too
large for me to download.

**Current approach:**

I have scraped together my own tools and some open source software to
make this happen. It is an involved process that is not automatic, and
takes quite a bit of time running sequential processes that I’d rather
spend on the interesting research. I also can’t do this at the scale
that is required because the data is inherently messy. It requires time
on a large computing hub at my institution.

**With Data Commons Phase 1:**

Standards for interoperability of tools and harmonization of data and
metadata will let me run this in the cloud, and allow me to run the 
same workflow on all of my data. Automating these processes both saves
my time, and helps protect me from making errors.

**With Data Commons longer vision:**

Access to large datasets through single log-in. Data Commons will
provide a collaborative environment for research and development. Instead
of just using my idiosyncratic method, I can try validated workflows 
created by others. This will help me ensure that my results are robust
and reproducible, and will also allow me to try analysis that are outside
of my abiity to program myself.
