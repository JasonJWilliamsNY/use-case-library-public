---
title: Analysis at Scale
narratives:
---

**Scenario:**

I am working with a really large and complex dataset. Technically, it
is multiple datasets stored in different places. I have a workflow
that I run on this dataset and I’d like to update it as new data is
added. I don’t have the memory or compute power to do this as a robust
process. And let’s not even talk about the data.

**Current approach:**

I have scraped together my own tools and some open source software to
make this happen. It is an involved process that is not automatic,
takes quite a bit of time running sequential processes that I’d rather
spend on the interesting research. I also can’t do this at the scale
that is required because the data is inherently messy. It is easier
when I get time on a supercomputing hub.

**With Data Commons Phase 1:**

Standards for interoperability of tools and harmonization of data and
metadata.

**With Data Commons longer vision:**

Access to large datasets through single log-in. Data Commons will
provide a collaborative environment for research and development. The
new ideas and tools from the community will be consumed in the Data
Commons and contribute to its long term value and sustainability. A
built in user community.
